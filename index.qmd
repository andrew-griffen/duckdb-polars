---
title: Big data work with DuckDB, Polars, and friends
subtitle: Better tools for bigger problems
format:
  html:
    toc: true
    toc-depth: 4
    self-contained: true
author:
  - name: Grant McDermott
    email: gmcd@amazon.com
    affiliations: Principal Economist, GMAC Econ
date: last-modified
execute: 
  cache: true
---

:::{.callout-note}
## Author note:

_This tutorial is adapted from a internal workshop that I gave for Amazon
Economics University.^[AEU is an in-house training program, focused on technical
skills for economists (and related job families) at Amazon.] The material has
been slightly shortened and all sensitive business context has been removed. We
only use publicly available data in the examples that follow. Here is the
original workshop abstract._

**Abstract:** This session will introduce you to [DuckDB](https://duckdb.org/)
and [Polars](https://github.com/pola-rs/polars), two data wrangling libraries
at the frontier of high-performance computation. (See
[benchmarks](https://duckdblabs.github.io/db-benchmark/).) In addition to being
extremely fast and lightweight, both DuckDB and Polars provide user-friendly
implementations across multiple languages. This makes them very well suited to
the type of work that economist teams do at Amazon. We will provide a variety
of real-life examples in both Python and R, with the aim of getting
participants up and running as quickly as possible. We will also discuss some
complementary tools and how these can be integrated for an efficient end-to-end
workflow (data I/O -> wrangling -> analysis).
:::


## Preliminaries

### Data

The examples in this tutorial make use of the ~~infamous~~
well-known [New York City
taxi](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page) data. 

- Just a small portion of it, to tell the truth. But enough to demonstrate the point. 
- The final dataset is ~8.5 GB compressed on disk and can take 10-20 minutes to download, depending on your internet connection.


You
can download the dataset with the below terminal commands.

:::{.callout-note}
You will need the `aws cli` tool ([install link](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html)) for these next commands to work. See further below for some alternative download options.
:::

```sh
mkdir -p nyc-taxi/year=2012
aws s3 cp s3://voltrondata-labs-datasets/nyc-taxi/year=2012 nyc-taxi/year=2012 --recursive
```

Again, depending on your internet connection, this can 10 to 20 minutes to download. (If you are
on a cloud dekstop or EC2 machine that is located in the same US-east-2 region, then it should be
much quicker.) Feel free to only grab a smaller subset if you don't want to wait
this long, e.g.

```sh
mkdir -p nyc-taxi/year=2012/month=1
mkdir -p nyc-taxi/year=2012/month=2
aws s3 cp s3://voltrondata-labs-datasets/nyc-taxi/year=2012/month=1/ nyc-taxi/year=2012/month=1 --recursive
aws s3 cp s3://voltrondata-labs-datasets/nyc-taxi/year=2012/month=2/ nyc-taxi/year=2012/month=2 --recursive
```

#### Alternative download options

If you don't have the `aws cli` tool, or can't install install it for some
reason, then you can always download the dataset directly from R or Python using
some of the packages that we installed above. For example:

```r
library(arrow)
library(dplyr)

data_path = "nyc-taxi/year=2012" # Or set your own preferred path

open_dataset("s3://voltrondata-labs-datasets/nyc-taxi/year=2012") |>
    write_dataset(data_path, partitioning = "month")
```


### Libraries

::: {.panel-tabset}

## R

Run the following commands in your R console.

```r
install.packages(c("duckdb", "arrow", "dplyr", "tidyr"))
```

We also have different install targets for **r-polars**, depending on your operating system.

```r
## MacOS / Windows
install.packages("polars", repos = "https://rpolars.r-universe.dev")
## Ubuntu
install.packages("polars", repos = "https://rpolars.r-universe.dev/bin/linux/jammy/4.3")
```

## Python

Run the following commands in your terminal.

```sh
pip3 install polars duckdb pyarrow pandas
pip3 install 'ibis-framework[duckdb]'
```

:::


## DuckDB

::: {.panel-tabset}

## R

```{r}
#| cache: false

library(duckdb)
```

## Python

```{python}
#| cache: false

import duckdb
import time # just for timing some queries
```

:::


### Native SQL API

The first thing we need to do is instantiate a connection with an _in-memory_ database.

::: {.panel-tabset}

## R

```{r con}
#| cache: false
 
con = dbConnect(duckdb(), shutdown = TRUE)
```

Aside: The `shutdown = TRUE` argument is a convenience feature that ensures our
connection is automatically terminated when our R session ends (i.e., even if we
forget to do it manually.) I'm not aware of a similar convenience argument for
Python; please let me know if I am missing something.

## Python

```{python con_py}
#| cache: false

con = duckdb.connect(database = ':memory:', read_only = False)
```

:::

The fact that this connection lives in memory is a killer feature of DuckDB (one that it inherits from SQLite). We don't need to connect to some complicated, existing database to harness all of DuckDB's power. Instead we can just spin up an ephemeral database that interacts directly with our R or Python (or Julia, etc.) client.
At the same time, however, it's worth noting that you _can_ create a persistent, disk-backed database simply by providing an appropriate path as part of your connection. This also enables out-of-core computation for
bigger than RAM data.


::: {.panel-tabset}

## R

```r
## Uncomment and run the next line if you'd like to create a persistent,
## disk-backed database instead.

# con = dbConnect(duckdb(), dbdir = "nyc.duck")
```

## Python

```python
## Uncomment and run the next line if you'd like to create a persistent,
## disk-backed database instead.

# con = duckdb.connect("nyc.duck")
```

:::

Aside: The `".duck"` file extension above is optional. You could also use `".db"`, `".dbb"`, etc.

#### Aggregations

Vanilla SQL syntax just to demonstrate (This works fine but, I'll invoke DuckDB's nicer syntax support for the timed queries below.)

::: {.panel-tabset}

## R

```{r db_sql_vanilla}
dbGetQuery(
  con,
  "
    SELECT
      passenger_count,
      AVG(tip_amount) AS mean_tip
    FROM
      'nyc-taxi/**/*.parquet'
    GROUP BY
      passenger_count
    "
)
```

## Python

```{python db_sql_vanilla_py}
con.execute(
    '''
    SELECT
      passenger_count,
      AVG(tip_amount) AS mean_tip
    FROM
      'nyc-taxi/**/*.parquet'
    GROUP BY
      passenger_count
    '''
).df()
```

:::

NB: I'm going to use some of DuckDB's friendlier SQL syntax from now on.
See: https://duckdb.org/2023/08/23/even-friendlier-sql.html

::: {.panel-tabset}

## R

```{r db_sql_dat1}
tic = Sys.time()
dat1 = dbGetQuery(
  con,
  "
    FROM
      'nyc-taxi/**/*.parquet'
    SELECT
      passenger_count,
      AVG(tip_amount) AS mean_tip
    GROUP BY ALL
    "
)
(toc = Sys.time() - tic)

dat1
```

## Python

```{python db_sql_dat1_py}
tic = time.time()
dat1 = con.execute(
    '''
    FROM
      'nyc-taxi/**/*.parquet'
    SELECT
      passenger_count,
      AVG(tip_amount) AS mean_tip
    GROUP BY ALL
    '''
).df()
toc = time.time() - tic
print(f"Time difference of {toc} seconds.")

dat1
```

:::

Subsetting along partition dimensions allows for even more efficiency gains.

::: {.panel-tabset}

## R

```{r db_sql_dat2}
tic = Sys.time()
dat2 = dbGetQuery(
    con,
    "
    FROM 'nyc-taxi/**/*.parquet'
    SELECT
      month,
      passenger_count,
      AVG(tip_amount) AS mean_tip
    WHERE month <= 3
    GROUP BY ALL
    "
    )
(toc = Sys.time() - tic)
```

## Python

```{{python db_sql_dat2}}
tic = time.time()
dat2 = con.execute(
    '''
    FROM 'nyc-taxi/**/*.parquet'
    SELECT
      month,
      passenger_count,
      AVG(tip_amount) AS mean_tip
    WHERE month <= 3
    GROUP BY ALL
    '''
).df()
toc = time.time() - tic
print(f"Time difference of {toc} seconds.")
```

:::

Another example (incl. one high-dimensional grouping column; i.e. `trip_distance`)

::: {.panel-tabset}

## R

```{r db_sql_dat3}
tic = Sys.time()
dat3 = dbGetQuery(
  con,
  "
    FROM 'nyc-taxi/**/*.parquet'
    SELECT
      passenger_count,
      trip_distance,
      AVG(tip_amount) AS mean_tip,
      AVG(fare_amount) AS mean_fare
    GROUP BY ALL
  "
)
(toc = Sys.time() - tic)

nrow(dat3)
head(dat3)
```

## Python

```{{python db_sql_dat3}}
dat3 = (
    con.execute(
        '''
        FROM read_parquet(
        'nyc-taxi/**/*.parquet',
        HIVE_PARTITIONING=true
        )
        SELECT
        passenger_count,
        trip_distance,
        AVG(tip_amount) AS mean_tip,
        AVG(fare_amount) AS mean_fare
        GROUP BY ALL
        '''
    )
    .df()
)

len(dat3)
dat3.head()
```

:::


#### Pivot (reshape)

Let's explore some pivot (reshape) examples, building off the previous query.

- `UNPIVOT` = reshape from wide to long
- `PIVOT` = reshape from long to wide.

Here I'll use a Common Table Expression (CTE) to define a temporary table before unpivoting---i.e., reshaping long---at the end.

::: {.panel-tabset}

## R

```{r db_sql_pivot}
tic = Sys.time()
ldat = dbGetQuery(
  con,
  "
    WITH tmp_table AS (
      FROM 'nyc-taxi/**/*.parquet'
      SELECT
        passenger_count,
        trip_distance,
        AVG(tip_amount) AS mean_tip,
        AVG(fare_amount) AS mean_fare
      GROUP BY ALL
    )
    UNPIVOT tmp_table
    ON mean_tip, mean_fare
    INTO
      NAME variable
      VALUE amount
  "
)
(toc = Sys.time() - tic)

head(ldat)
```

## Python

:::

Another option would have been to create a new table in memory
and then pivot over that, which segues nicely to...

#### Digression: Create new tables

CTEs are a very common, and often efficient, way to implement multi-table
operations in SQL. But, for the record, We can create new tables in DuckDB's
memory cache pretty easily using `CREATE TABLE new_table_name AS ...`

::: {.panel-tabset}

## R

```{r, db_sql_taxi2}
dbExecute(
  con,
  "
    CREATE TABLE taxi2 AS
    FROM 'nyc-taxi/**/*.parquet'
    SELECT
      passenger_count,
      trip_distance,
      AVG(tip_amount) AS mean_tip,
      AVG(fare_amount) AS mean_fare
    GROUP BY ALL
  "
)

dbListTables(con)
```

FWIW, you can always remove a table with `dbRemoveTable()`.

## Python


:::

#### Back to reshaping

With our new `taxi2` table in hand, let's redo the previous unpivot query
directly on this new table. This makes the actual (un)pivot statement a bit
clearer... and also separates out the execution time.

::: {.panel-tabset}

## R

```{r db_sql_ldat2}
dbGetQuery(
  con,
  "
    UNPIVOT taxi2
    ON mean_tip, mean_fare
    INTO
      NAME variable
      VALUE amount
    LIMIT 10
  "
)
```

## Python

:::

(Note how crazy fast pivoting in DuckDB actually is.)

#### Joins (merges)

::: {.panel-tabset}

## R

```{r db_sql_join}
dbGetQuery(
  con,
  "
    WITH 
      mean_tips AS (
        FROM 'nyc-taxi/**/*.parquet'
        SELECT
          month,
          AVG(tip_amount) AS mean_tip
        GROUP BY month
      ),
      mean_fares AS (
        FROM 'nyc-taxi/**/*.parquet'
        SELECT
          month,
          AVG(fare_amount) AS mean_fare
        GROUP BY month 
      )
    FROM mean_tips
    LEFT JOIN mean_fares
    USING (month)
    SELECT *
    ORDER BY mean_tips.month
  "
)
```

## Python

:::

:::{.callout-tip}
## Challenge

Redo the above join but, rather than using CTEs, use tables that you
first create in DuckDB's memory bank. Again, this will simplify the actual join operation and also emphasise how crazy fast joins are in DuckDB.
:::


#### Close connection

::: {.panel-tabset}

## R

```{r db_con_close}
#| cache: false

dbDisconnect(con)
```

Again, this step isn't strictly necessary since we instantiated our connection with the `shutdown = TRUE` argument (way back here). But it's worth seeing in case you want to be explicit.

## Python

```{python db_con_close_py}
#| cache: false

con.close()
```

:::