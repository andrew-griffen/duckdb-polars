{
  "hash": "71617f8e14bfb253a54925e59cad4312",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Hands on with DuckDB + dplyr\nexecute:\n  freeze: auto\n  cache: true\n---\n\n\n## Load libraries\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(duckdb)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: DBI\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(dplyr)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'dplyr'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(tidyr)\n```\n:::\n\n\n## Create a database connection\n\nFor the `d(b)plyr` workflow, the connection step is very similar to the pure SQL\napproach. The only difference is that, after instantiating the database\nconnection, we need to register our parquet dataset as a table in our connection\nvia the `dplyr::tbl()` function. Note that we also assign it to an object (here:\n`nyc`) that can be referenced from R.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Instantiate the in-memory DuckDB connection \ncon = dbConnect(duckdb(), shutdown = TRUE)\n\n## Register our parquet dataset as table in our connection (and that assign it\n## to an object that R understands)\n# nyc = tbl(con, \"nyc-taxi/**/*.parquet\") # works, but safer to use the read_parquet func)\nnyc = tbl(con, \"read_parquet('nyc-taxi/**/*.parquet', hive_partitioning = true)\")\n```\n:::\n\n\nThis next command runs instantly because all computation is deferred (i.e.,\nlazy eval). In other words, it is just a query object.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nq1 = nyc |>\n  summarize(\n    mean_tip = mean(tip_amount),\n    .by = passenger_count\n  )\n```\n:::\n\n\n:::{.callout-tip}\n## `.by` versus `group_by` \nIn case you weren't aware: `summarize(..., .by = x)` is a shorthand (and\nnon-persistent) version of `group_by(x) |> summarize(...)`. More details\n[here](https://www.tidyverse.org/blog/2023/02/dplyr-1-1-0-per-operation-grouping/).\n:::\n\nWe can see what DuckDB's query tree looks like by asking it to explain\nthe plan\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexplain(q1)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Missing values are always removed in SQL aggregation functions.\nUse `na.rm = TRUE` to silence this warning\nThis warning is displayed once every 8 hours.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<SQL>\nSELECT passenger_count, AVG(tip_amount) AS mean_tip\nFROM (FROM read_parquet('nyc-taxi/**/*.parquet', hive_partitioning = true)) q01\nGROUP BY passenger_count\n\n<PLAN>\nphysical_plan\n┌───────────────────────────┐\n│       HASH_GROUP_BY       │\n│   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n│             #0            │\n│          avg(#1)          │\n└─────────────┬─────────────┘                             \n┌─────────────┴─────────────┐\n│         PROJECTION        │\n│   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n│      passenger_count      │\n│         tip_amount        │\n└─────────────┬─────────────┘                             \n┌─────────────┴─────────────┐\n│       READ_PARQUET        │\n│   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n│      passenger_count      │\n│         tip_amount        │\n│   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n│       EC: 179629584       │\n└───────────────────────────┘                             \n```\n\n\n:::\n:::\n\n\nSimilarly, to show the SQL translation that will be implemented on the backend,\nusing `show_query`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshow_query(q1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<SQL>\nSELECT passenger_count, AVG(tip_amount) AS mean_tip\nFROM (FROM read_parquet('nyc-taxi/**/*.parquet', hive_partitioning = true)) q01\nGROUP BY passenger_count\n```\n\n\n:::\n:::\n\n\nNote that printing the query object actually does enforce some computation.\nOTOH it's still just a preview of the data (we haven't pulled everything into\nR's memory)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nq1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Source:   SQL [?? x 2]\n# Database: DuckDB v0.10.1 [gmcd@Darwin 23.4.0:R 4.4.0/:memory:]\n   passenger_count mean_tip\n             <dbl>    <dbl>\n 1               5    1.10 \n 2              65    0    \n 3               9    0.807\n 4             177    1    \n 5               0    0.862\n 6             254    0    \n 7             249    0    \n 8               7    0.544\n 9               8    0.351\n10              66    1.5  \n# ℹ more rows\n```\n\n\n:::\n:::\n\n\nTo actually pull all of the result data into R, we must call `collect()`\non the query object\n\n::: {.cell}\n\n```{.r .cell-code}\ntic = Sys.time()\ndat1 = collect(q1)\ntoc = Sys.time()\n\ndat1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 18 × 2\n   passenger_count mean_tip\n             <dbl>    <dbl>\n 1               0    0.862\n 2             254    0    \n 3             249    0    \n 4               5    1.10 \n 5              65    0    \n 6               9    0.807\n 7             177    1    \n 8               2    1.08 \n 9             208    0    \n10              10    0    \n11               4    0.845\n12               1    1.15 \n13             247    2.3  \n14               3    0.963\n15               6    1.13 \n16               7    0.544\n17               8    0.351\n18              66    1.5  \n```\n\n\n:::\n\n```{.r .cell-code}\ntoc - tic\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTime difference of 1.2924 secs\n```\n\n\n:::\n:::\n\n\n\n\n## Aggregation\n\nHere's our earlier filtering example with multiple grouping + aggregation\nvariables...\n\n\n::: {.cell}\n\n```{.r .cell-code}\nq2 = nyc |>\n  filter(month <= 3) |>\n  summarize(\n    across(c(tip_amount, fare_amount), mean),\n    .by = c(month, passenger_count)\n  )\nq2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Source:   SQL [?? x 4]\n# Database: DuckDB v0.10.1 [gmcd@Darwin 23.4.0:R 4.4.0/:memory:]\n   month passenger_count tip_amount fare_amount\n   <dbl>           <dbl>      <dbl>       <dbl>\n 1     1               1      1.04         9.76\n 2     2               1      1.07         9.90\n 3     3               1      1.09        10.2 \n 4     1               8      0           21.3 \n 5     2               8      0.5          8.23\n 6     1               7      0            6.3 \n 7     1               3      0.875        9.87\n 8     1               6      1.02         9.86\n 9     2               3      0.895        9.98\n10     2               6      1.02         9.96\n# ℹ more rows\n```\n\n\n:::\n:::\n\n\nAside: note the optimised query includes hash groupings and projection\n(basically: fancy column subsetting, which is a suprisingly effective strategy\nin query optimization)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexplain(q2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<SQL>\nSELECT\n  \"month\",\n  passenger_count,\n  AVG(tip_amount) AS tip_amount,\n  AVG(fare_amount) AS fare_amount\nFROM (\n  SELECT q01.*\n  FROM (FROM read_parquet('nyc-taxi/**/*.parquet', hive_partitioning = true)) q01\n  WHERE (\"month\" <= 3.0)\n) q01\nGROUP BY \"month\", passenger_count\n\n<PLAN>\nphysical_plan\n┌───────────────────────────┐\n│       HASH_GROUP_BY       │\n│   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n│             #0            │\n│             #1            │\n│          avg(#2)          │\n│          avg(#3)          │\n└─────────────┬─────────────┘                             \n┌─────────────┴─────────────┐\n│         PROJECTION        │\n│   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n│           month           │\n│      passenger_count      │\n│         tip_amount        │\n│        fare_amount        │\n└─────────────┬─────────────┘                             \n┌─────────────┴─────────────┐\n│       READ_PARQUET        │\n│   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n│      passenger_count      │\n│        fare_amount        │\n│         tip_amount        │\n│           month           │\n│   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n│ File Filters: (month <= 3)│\n│   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n│        EC: 44907396       │\n└───────────────────────────┘                             \n```\n\n\n:::\n:::\n\n\nAnd our high-dimensional aggregation example. We'll create a query for this\nfirst, since I'll reuse it shortly again\n\n\n::: {.cell}\n\n```{.r .cell-code}\nq3 = nyc |>\n  group_by(passenger_count, trip_distance) |>\n  summarize(\n    across(c(tip_amount, fare_amount), mean),\n  ) \ncollect(q3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 25,569 × 4\n# Groups:   passenger_count [18]\n   passenger_count trip_distance tip_amount fare_amount\n             <dbl>         <dbl>      <dbl>       <dbl>\n 1               1          0.7       0.493        5.04\n 2               2          0.8       0.462        5.44\n 3               1          0.8       0.535        5.36\n 4               1          1         0.616        6.01\n 5               1          0.3       0.334        4.11\n 6               1          4.8       1.70        16.0 \n 7               2          1         0.525        6.08\n 8               1          6.7       2.13        20.1 \n 9               1          0.4       0.361        4.17\n10               2          0.98      0.542        5.85\n# ℹ 25,559 more rows\n```\n\n\n:::\n:::\n\n\n## Pivot (reshape)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# library(tidyr) ## already loaded\n\nq3 |>\n  pivot_longer(tip_amount:fare_amount) |>\n  collect()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 51,138 × 4\n# Groups:   passenger_count [18]\n   passenger_count trip_distance name       value\n             <dbl>         <dbl> <chr>      <dbl>\n 1               1           2.2 tip_amount 1.01 \n 2               1          16.8 tip_amount 5.58 \n 3               1           2.7 tip_amount 1.16 \n 4               1           5.9 tip_amount 1.93 \n 5               1           8.4 tip_amount 2.93 \n 6               0           2.4 tip_amount 0.924\n 7               1           3.8 tip_amount 1.46 \n 8               1           5.4 tip_amount 1.83 \n 9               1           9.3 tip_amount 3.40 \n10               1           9.8 tip_amount 3.60 \n# ℹ 51,128 more rows\n```\n\n\n:::\n:::\n\n\n## Joins (merges)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean_tips  = nyc |> summarise(mean_tips = mean(tip_amount), .by = month)\nmean_fares = nyc |> summarise(mean_fares = mean(fare_amount), .by = month)\n```\n:::\n\n\nAgain, these commands complete instantly because all computation has been\ndeferred until absolutely necessary (i.e.,. lazy eval).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nleft_join(\n  mean_fares,\n  mean_tips\n  ) |>\n  collect()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nJoining with `by = join_by(month)`\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 12 × 3\n   month mean_fares mean_tips\n   <dbl>      <dbl>     <dbl>\n 1     5      10.6       1.08\n 2     9      12.4       1.25\n 3     1       9.81      1.01\n 4    11      12.3       1.25\n 5    12      12.3       1.24\n 6     3      10.2       1.06\n 7     6      10.5       1.09\n 8    10      12.5       1.28\n 9     2       9.94      1.04\n10     4      10.3       1.04\n11     7      10.4       1.06\n12     8      10.5       1.08\n```\n\n\n:::\n:::\n\n\n## Windowing\n\nIf you recall from the native SQL API, we sampled 1 percent of the data before\ncreating decile bins to reduce the computation burden of sorting the entire\ntable. Unfortunately, this approach doesn't work as well for the **dplyr**\nfrontend because the underlying SQL translation\n[uses](https://dbplyr.tidyverse.org/reference/dbplyr-slice.html) a generic\nsampling approach (rather than DuckDB's optimised `USING SAMPLE` statement.)\n\n## Close connection\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndbDisconnect(con)\n```\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}